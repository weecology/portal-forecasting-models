\documentclass{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[autostyle]{csquotes}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[T1]{fontenc}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{indentfirst}
\usepackage{bbold}
\def\code#1{\texttt{#1}}
\hypersetup{
    colorlinks = true,
    linkcolor = black,
    filecolor = black,      
    urlcolor = black,
    citecolor = black,
}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

\usepackage{geometry}
\geometry{legalpaper, portrait, margin=1in}

\title{Portal Forecasting Models}


\author[1]{Weecology Laboratory}
\affil[1]{University of Florida}

\date{\today}

\begin{document}
\maketitle
\tableofcontents

\addcontentsline{toc}{section}{Overview}
\section*{Overview}
\label{sec:overview}

This document describes the models used to forecast ecological predictions for the Portal Project \citep{Brown1998, PortalPredictions}. 

\section{Current Models}
\label{sec:currmods}

The Portal rodent data are currently forecast using four models: ESSS, AutoArima, nbGARCH, and pevGARCH, each of which is explained below. Three of the four models (ESSS, nbGARCH, and pevGARCH) assume evenly-spaced sampling of the time series, yet the rodents were not censused on 16.5\% new moons. To facilitate comparison among the models, we therefore treated all four of them as if they required evenly-spaced observations, and interpolated missing data (using simple linear interpolation) prior to analyses. 

\subsection{ESSS}
\label{subsec:currmods_esss}

ESSS is a flexible exponential smoothing state space model \citep{Hyndman2008} fit to the data at the composite (site) spatial level and the composite (community) ecological level. The model is selected and fitted using the \code{ets} and \code{forecast} functions in the \textbf{forecast} package \citep{Hyndman2017} with the \code{allow.multiplicative.trend} argument set to \code{TRUE} and the \code{forecast\_esss} function in our \textbf{portalPredictions} package \citep{PortalPredictions}. Models fit using \code{ets} implement what is known as the "innovations" approach to state space modeling, which assumes a single source of noise that is equivalent for the process and observation errors \citep{Hyndman2008}.

In general, ESSS models are defined according to three model structure parameters: error type, trend type, and seasonality type \citep{Hyndman2008}. Each of the parameters can be an N (none), A (additive), or M (multiplicative) state \citep{Hyndman2008}. However, because of the difference in period between seasonality and sampling of the Portal rodents combined with the hard-coded single period of the \code{ets} function, we could not include the seasonal components to the ESSS model. ESSS is fit flexibly, such that the model parameters can vary from fit to fit.

\subsection{AutoArima}
\label{subsec:currmods_aa}

AutoArima is a flexible Auto-Regressive Integrated Moving Average (ARIMA) model fit to the data at the composite (site) spatial level and the composite (community) ecological level. The model is selected and fitted using the \code{auto.arima} (with the \code{lambda} argument set to 0) and \code{forecast} functions in the \textbf{forecast} package \citep{Hyndman2013, Hyndman2017} and the \code{forecast\_autoarima} function in our \textbf{portalPredictions} package \citep{PortalPredictions}.

Generally, ARIMA models are defined according to three model structure parameters: the number of autoregressive terms (p), the degree of differencing (d), and the order of the moving average (q), and are represented as ARIMA(p, q, q) \citep{Box1970}. While the \code{auto.arima} function allows for seasonal models, the seasonality is hard-coded to be on the same period as the sampling, which is not the case for the Portal rodent surveys. As a result, no seasonal models were evaluated. AutoArima is fit flexibly, such that the model parameters can vary from fit to fit.

\subsection{nbGARCH}
\label{subsec:currmods_nbgarch}

nbGARCH is a generalized autoregressive conditional heteroskedasticity (GARCH) model with overdispersion (\emph{i.e.}, a negative binomial response) fit to the data at the composite (site) spatial level and both the composite (community) and the articulated (species) ecological levels. The model for each species and the community total is selected and fitted using the \code{tsglm} function in the \textbf{tscount} package \citep{Liboschik2017a} and the \code{forecast\_nbgarch} function in our \textbf{portalPredictions} package \citep{PortalPredictions}.

GARCH models are generalized ARMA models and are defined according to their link function, response distribution, and two model structure parameters: the number of autoregressive terms (p) and the order of the moving average (q), and are represented as GARCH(p, q) \citep{Liboschik2017a}. The nbGARCH model is fit using the log link and a negative binomial response (modeled as an over-dispersed Poisson), as well as with p = 1 (first-order autoregression) and q = 12 (approximately yearly moving average).

The \code{tsglm} function in the \textbf{tscount} package \citep{Liboschik2017a} uses a (conditional) quasi-likelihood based approach to inference and models the overdispersion as an additional parameter in a two-step approach. This two-stage approach has only been minimally evaluated, although preliminary simulation-based studies are promising \citep{Liboschik2017b}.    

\subsection{pevGARCH}
\label{subsec:currmods_pevgarch}

pevGARCH is a generalized autoregressive conditional heteroskedasticity (GARCH) model fit to the data at the composite (site) spatial level and both the composite (community) and the articulated (species) ecological levels. The response variable is Poisson, and a variety of environmental variables are considered as covariates. The model for each species is selected and fitted using the \code{tsglm} function in the \textbf{tscount} package \citep{Liboschik2017a} and the \code{forecast\_pevgarch} function in our \textbf{portalPredictions} package \citep{PortalPredictions}.

GARCH models are generalized ARMA models and are defined according to their link function, response distribution, and two model structure parameters: the number of autoregressive terms (p) and the order of the moving average (q), and are represented as GARCH(p, q) \citep{Liboschik2017a}. The PETS model is fit using the log link and a Poisson response, as well as with p = 1 (first-order autoregression) and q = 12 (yearly moving average). The environmental variables potentially included in the model are min, mean, and max temperatures, precipitation, and NDVI. 

The \code{tsglm} function in the \textbf{tscount} package \citep{Liboschik2017a} uses a (conditional) quasi-likelihood based approach to inference. This approach has only been minimally evaluated for models with covariates, although preliminary simulation-based studies are promising \citep{Liboschik2017b}.  

Each species is fit using the following (nonexhaustive) sets of the environmental covariates:
\begin{itemize}
\item max temp, mean temp, precipitation, NDVI
\item max temp, min temp, precipitation, NDVI
\item max temp, mean temp, min temp, precipitation
\item precipitation, NDVI
\item min temp, NDVI
\item min temp
\item max temp
\item mean temp
\item precipitation 
\item NDVI
\item -none-
\end{itemize}

The final model is an intercept-only model. The single best model of the 11 is selected based on AIC. 

\section{Developing Model Framework}
\label{sec:devmods}

\subsection{Overview}
\label{subsec:devmods_ov}

The modeling approach proposed here is based on a data-assimilation approach to forecasting using dynamical state-space models (a.k.a. partially observed Markov process models), where the observed data are considered a partial sample of an underlying true state that evolves through time \citep{Ionides2006, King2016} and the models used to estimate and forecast the true state are iteratively evaluated and updated with each new observation. We use a suite of competing models to make inference about the historic and current state of the system and to produce forecasts of expected future observations. In addition, statistically weighted ensemble models are constructed based on long-running model accuracy. Ongoing data collections provide new observations, which are used to iteratively evaluate the models and produce new predictions via the forecast cycle \citep{Dietze2017}. 

Recognizing the evolving nature of forecast model suites, however, we broaden Deitze's (\citeyear{Dietze2017}) forecast cycle to accommodate a dynamic model set while still remaining a focused project. We consider the following steps of a forecasting project:

\begin{enumerate}
\item{Define question of interest} 
\item{Identify best metric(s) to inform [1]}
\item{Propose potential influences on [2]} 
\item{Construct models that represent [3]}
\item{Statistically fit parameter distributions within [4] using the available data}
\item{Produce forecasts using [4] at [5]} 
\item{Collect new data}
\item{Evaluate [6] based on [7]}
\end{enumerate}

Evaluations in step $[8]$ are the foundation of constructing ensembles of models as well as determining if additional models need to be added to $[4]$ and--if needed--can be used to ultimately remove specific models from $[4]$. 

After completion of $[8]$ for a given set of predictions and observations, the cycle loops back to at least $[5]$, updating the parameter distributions based on the new data. It is possible to loop back to $[3]$ or $[4]$ (by proposing a new influence or a new model formulation, respectively), but not to earlier than $[3]$, thereby ensuring that the models being used within the project are all answering the same question using the same metric (\emph{i.e.}, the same response variable). It is expected that the process of forecasting will generate new questions and metrics to evaluate, but we consider loops back to $[1]$ or $[2]$ to be significant enough (\emph{e.g.}, existing models would need to be reconfigured to predict different response data) to warrant treatment as a new project.

\subsection{General Framework and Notation}
\label{subsec:devmods_gf}

We use $y$ to represent a singular observation and $y_{ijk}$ is the observed number of rodents of species $i$ in plot $j$ at survey $k$. Because $y_{ijk}$ is a count of individuals, the variable is non-negative and integer-valued. The observed numbers of individuals across each species $i$ in $1 \ldots I$ and each plot $j$ in $1 \ldots J$ for a given survey $k$ is an $I \times J$ matrix $\textbf{Y}_k$. The series of observations for surveys $1 \ldots K$ is given by $\textbf{Y}_{1:K}$ (an $I \times J \times K$ array). We use $k$ as an index notation to represent discrete observation events (surveys) that occur through continuous time $t$, where $t_k$ refers to the time of survey $k$.

Our specific observations are noted with a superscripted asterisk ($y^{*}$): the number of individuals of species $i$ actually observed in plot $j$ during survey $k$ is $y^{*}_{ijk}$, the head-counts of all species in all plots observed during survey $k$ is $\textbf{Y}^{*}_{k}$, and the series of actual observations is $\textbf{Y}^{*}_{1:K}$. A future observation (\emph{i.e.}, what is to be forecast) is denoted $\tilde{y}$, such that the number of species $i$ in plot $j$ that will exist at the next observation is $\tilde{y}_{ij(k+1)}$, the number of all species in all plots $n$ observations into the future is $\tilde{\textbf{Y}}_{k+n}$, and the series of $N$ future observations is $\tilde{\textbf{Y}}_{(k+1):(k+N)}$. Combining these notations, our future specific data are represented by both a tilde and an asterisk. An individual datum would be $\tilde{y}^{*}_{ij(k+n)}$ and the set of specific future data is $\tilde{\textbf{Y}}^{*}_{(k+1):(k+N)}$.

The suite of $M$ competing models is compiled to forecast $\tilde{\textbf{Y}}_{(k+1):(k+N)}$ based on $\textbf{Y}_{1:K}$. Each model ($\mathcal{M}_m$, for $m$ in $1 \ldots M$) is defined by three functions that describe the evolution of the underlying state and the data collected on it. $f_{y_k|z_k}$ is a general observation function that translates the true underlying state ($\textbf{Z}_k$, number or density of each species in each plot, following the same structure and subscript notation as $\textbf{Y}_k$) into the number of each species actually observed in each plot ($\textbf{Y}_k$):

\begin{equation}
\label{eq:1}
\textbf{Y}_k = f_{y_k|z_k}(\textbf{Z}_k, \textbf{X}_{(y|z), k}, \Theta_{(y|z), k})
\end{equation}

where $\textbf{X}_{(y|z), k}$ are detection-associated covariates and $\Theta_{(y|z), k}$ are detection-function parameters. Both $\textbf{X}_{(y|z), k}$ and $\Theta_{(y|z), k}$ are $J$-column matrices (representing the number of plots), but with different numbers of rows than $\textbf{Y}_k$ and $\textbf{Z}_k$ (both of which have $I$ rows), reflecting the fact that they contain different characteristics (\emph{i.e.}, covariates, parameters). 

The unobserved true state ($\textbf{Z}$) is a multi-dimensional, evolving, and non-negative variable that could be integer-valued (if the state is represented by the number of individuals) or continuous (if the state is represented by the density of individuals). In either case, we use a general process model $f_{z_k|z_{k-1}}$ that describes the evolution of $\textbf{Z}$ from one time point ($k-1$) to the next ($k$):

\begin{equation}
\label{eq:2}
\textbf{Z}_{k} = f_{z_k|z_{k-1}}(\textbf{Z}_{k-1}, \Delta t_{(k-1):k}, \textbf{X}_{(z_k|z_{k-1}), k},\Theta_{(z_k|z_{k-1}), k})
\end{equation}

where $\Delta t_{(k-1):k}$ is the time step ($t_k - t_{k-1}$), $\textbf{X}_{(z_k|z_{k-1}), k}$ are process-associated covariates, and $\Theta_{(z_k|z_{k-1}), k}$ are process-function parameters. $\textbf{X}_{(z_k|z_{k-1}), k}$ and $\Theta_{(z_k|z_{k-1}), k}$ are both $J$-column matrices, corresponding to  the number of plots, but they have different numbers of rows than $\textbf{Y}_k$ and $\textbf{Z}_k$ (both of which have $I$ rows), because they contain covariates and parameters, rather than data on species. $f_{z_k|z_{k-1}}$ is general; it can be phenomenological or mechanistic, but it must generate appropriate values for $\textbf{Z}_{k}$. 

Given the evolutionary aspect of the model, we must define a starting point for $\textbf{Z}$: $\textbf{Z}_{k=0}$. To do so, we use the general initialization function $f_{\textbf{Z}_0}$, which only depends upon initialization-associated covariates $\textbf{X}_{z_0}$ and parameters $\Theta_{z_0}$:

\begin{equation}
\label{eq:3}
\textbf{Z}_{k=0} = f_{z_0}(\textbf{X}_{z_0}, \Theta_{z_0})
\end{equation}

$\textbf{X}_{(z_0)}$ and $\Theta_{(z_0)}$ are $J$-column matrices (one column for each plot), with the number of rows corresponding to the number of initialization-associated covariates and parameters, respectively. 

Thus, each model ($\mathcal{M}_m$, for $m$ in $1 \ldots M$) is defined by observation, process evolution, and initialization functions, notated as $f^m_{y_k|z_k}$, $f^m_{y_k|z_k}$, and $f^m_{\textbf{Z}_0}$, respectively (note: the $m$'s are superscripted index values, not exponents). This is a general framework into which any state-space model can be placed, as long as it generates non-negative integer values for observations $y$. 

\subsection{Likelihood Estimation}
\label{subsec:devmods_le}

Likelihood forms the basis of model-based statistical inference \citep{Pawitan2001} and is a key component in our evaluation and use of our forecasts. The total likelihood of a particular model ($\mathcal{M}_m$) given the specific set of data actually observed ($\textbf{Y}^{*}_{1:K}$) is written $\mathcal{L}(\mathcal{M}_m|\textbf{Y}^{*}_{1:K})$. This likelihood is the data-based, long-running measure of support for that model and is the product of the conditional likelihoods of the model and all previous observations, given each observation:

\begin{equation}
\label{eq:4}
\mathcal{L}(\mathcal{M}_m|\textbf{Y}^{*}_{1:K}) = \prod_{k = 1}^{K}{\mathcal{L}_k(\mathcal{M}_m, \textbf{Y}^{*}_{1:k-1}|\textbf{Y}^{*}_{k})},
\end{equation}

which is equal to the product of the probabilities of each observation, given the model and the history of observations to that point:

\begin{equation}
\label{eq:5}
\mathcal{L}(\mathcal{M}_m|\textbf{Y}^{*}_{1:K}) = \prod_{k = 1}^{K}{\mathbb{P}[\textbf{Y}^{*}_{k}|\mathcal{M}_m, \textbf{Y}^{*}_{1:k-1}]}
\end{equation}

According to Eq. \ref{eq:5}, in order for us to determine the likelihood of the model, we must be able to calculate the probability of each observation ($\textbf{Y}^{*}_{k}$) given the history of previous observations ($\textbf{Y}^{*}_{1:(k-1)}$) and the model ($\mathcal{M}_m$). However, given our usage of a state-space model, $Y$ is conditionally independent: there is no direct relationship between $\textbf{Y}^{*}_{1:(k-1)}$ and $\textbf{Y}^{*}_{k}$. Rather, we rely on the indirect relationship between  observations $\textbf{Y}^{*}_{k-1}$ and $\textbf{Y}^{*}_{k}$ that occurs via the partially observed states at those times ($\textbf{Z}_{k-1}$ and $\textbf{Z}_{k}$) (Eq.\ref{eq:2}). 

When evaluating the observations at a given time point $k$, we use Bayes' theorem  to infer the state at $k-1$ ($\textbf{Z}_{k-1}$) from the history of observations $\textbf{Y}^{*}_{1:(k-1)}$, then project the state forward to $k$ using the process evolution function, and finally transform the state $\textbf{Z}_{k}$ to the observation at $k$, $\textbf{Y}^{*}_{k}$. We factorize the point-specific probability accordingly:

\begin{equation}
\label{eq:6}
\mathbb{P}[\textbf{Y}^{*}_{k}|\mathcal{M}_m, \textbf{Y}^{*}_{1:k-1}] = \sum_{\textbf{Z}_k}{\mathbb{P}[\textbf{Y}^*_k|\textbf{Z}_k, \mathcal{M}_m]}\sum_{\textbf{Z}_{k-1}}{\mathbb{P}[\textbf{Z}_k|\textbf{Z}_{k-1}, \mathcal{M}_m]\mathbb{P}[\textbf{Z}_{k-1}|\textbf{Y}^*_{1:(k-1)}, \mathcal{M}_m]}
\end{equation}

In order to retain generality regarding model structure, we leverage flexible, simulation-based approaches to estimate model likelihood, thereby allowing us to avoid evaluating probability densities directly \citep{Breto2009, He2010, Ionides2015}. Specifically, we rely on sequential Monte Carlo (SMC) methods, often referred to as \emph{particle filtration} to numerically estimate likelihoods \citep{Kitagawa1987, Doucet2001, Ionides2006}

For notation, we refer to $\textbf{Z}_{k}|\textbf{Y}^*_{1:k}$ as the \emph{filtering distribution} (at $k$) and $\textbf{Z}_{k}|\textbf{Y}^*_{1:(k-1)}$ as the \emph{prediction distribution} (at $k$). To estimate the probability of a specific observation (\emph{i.e.}, the likelihood of the model given that observation), we first draw (following Bayes' theorem) a set of $Q$ particles from the filtering distribution at the last observation ($k-1$), which we write as $\textbf{Z}^{F}_{(k-1),q}$, $q = 1, \ldots, Q$ (the $F$ is a superscript to refer to filtering, not an exponent). We then use the process evolution function (Eq. \ref{eq:2}) to simulate forward in time and generate the associated prediction distribution at $k$:

\begin{equation}
\label{eq:7}
\textbf{Z}^{P}_{k,q} \sim f_{z_k|z_{k-1}}(\textbf{Z}^{F}_{(k-1),q}, \Delta t_{(k-1):k}, \textbf{Z}_{(z_k|z_{k-1}), k},\Theta_{(z_k|z_{k-1}), k}), \; q = 1, \ldots, Q
\end{equation}

We then resample the prediction distribution with weights according to the observation function (Eq. \ref{eq:1}), which generates a distribution of $\textbf{Y}^{*}_{k}|\textbf{Z}_{k}$ given $\textbf{Y}^{*}_{1:(k-1)}$. Therefore, the conditional likelihood of the model given the observation at $k$ is

\begin{equation}
\label{eq:8}
\mathcal{L}_k(\mathcal{M}_m, \textbf{Y}^{*}_{1:(k-1)} | \textbf{Y}^{*}_{k}) \approx \frac{1}{Q}\sum_{q=1}^{Q}{\mathbb{P}[\textbf{Y}^*_k|\textbf{Z}^{P}_{k, q}, \mathcal{M}_m]}, \; q = 1, \ldots, Q
\end{equation}

We iterate Eq.\ref{eq:8} through the data from $k = 1$ to $k = K$, at which point we have obtained the conditional likelihood of each observation. These values are then multiplied together to achieve the likelihood of the model given the entire set of observations:

\begin{equation}
\label{eq:9}
\mathcal{L}(\mathcal{M}_m | \textbf{Y}^{*}_{1:K}) \approx \prod_{k = 1}^{K}{\frac{1}{Q}\sum_{q=1}^{Q}{\mathbb{P}[\textbf{Y}^*_k|\textbf{Z}^{P}_{k, q}, \mathcal{M}_m]}}, \; q = 1, \ldots, Q
\end{equation}

This equation forms the foundation of our statistical inference by describing how we generate an unbiased estimate of the likelihood for a model via simulation of the model and comparison to the observed data \citep{Doucet2001, King2016}.

\subsection{Defining Models}
\label{subsec:devmods_md}

Here we are using the term "model" in a general sense in that different models, say $\mathcal{M}_1$ and $\mathcal{M}_2$, could differ in parameter values but have the same structure, or they could differ in qualitative structure. In some situations (for example, producing an ensemble model or combing multiple parameter models within a structural model), we may be interested in subsuming models in a hierarchical sense such that a singular model $\mathcal{M}_m$ is actually a cluster of submodels $\mathcal{M}_{m_s}$ for $s$ in $1 \ldots S$. In such situations, each individual model would have a weight $w_{m_s}$, which reflects the contribution of that submodel $m_s$ to the model $m$. The sum of the weights ($\sum_{s = 1}^{S}{w_{m_s}}$) equals one, ensuring that the higher-level model is still a valid model with respect to the observations.

\subsection{Parameter Estimation}
\label{subsec:devmods_pe}

\subsection{Model Evaluation and Comparison}
\label{subsec:devmods_mec}

\subsection{Ensembles}
\label{subsec:devmods_ens}

\section{Specific Models}
\label{sec:mods}

Recall from above that each model ($\mathcal{M}_m$, for $m$ in $1 \ldots M$) is defined by observation, process evolution, and initialization functions: $f^m_{y_k|z_k}$, $f^m_{y_k|z_k}$, and $f^m_{\textbf{Z}_0}$. For the purposes of compactly listing models here, a specific model $\mathcal{M}_m$ encompasses parameter uncertainty within a qualitative structure (\emph{e.g.}, parameter inclusion or functional form). 

\bibliography{refs}  
\end{document}